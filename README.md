# Adult-Census-Income-Project
   <h6>Description and Motivation</h6>
                            <p>This was the second of two Capstone Projects as part of the Harvard Data Science Professional Certification. 
                                The objective was to predict, based on certain factors, whether a person's income is less than or greater than $50,000.
                                After having worked on a regression-based capstone project, 
                                I thought it would be interesting and quite useful to explore solutions to a classification-based problem.
                            </p>       
                            <h6>Business Value</h6>
                            <p>A classification model, a type of predictive model used to solve this analytical problem, 
                                is a cornerstone of supervised learning and is therefore extremely valuable in any business context. 
                                While there was no assigned business problem to this dataset, 
                                a sample use case could be that the federal government is looking to contact households who's income is above a certain threshold (for tax reasons as an example). 
                                Given that there will be a cost to false positive and false negatives, we can optimize our algorithms to minimize total cost.
                            </p>
                            <h6>Process</h6>
                            <ol>
                                <li>Exploratory Data Analysis/Data Visualization</li>
                                <li>Methods and Analysis - Feature Selection using RFE, Training 7 different classification-based models (with hyperparameter tuning)</li>
                                <li>Evaluation of Models - Testing and Validation</li>
                                <li>Conclusion</li>
                            </ol> 
                            <h6>Challenges</h6>
                            <p>
                                The primary challenge for me, just as it was in the MovieLens Project, was not to be overwhelmed by the sheer number of approaches and methods one could employ for each component of the methods and analysis section. 
                                There are a wide variety of ways, for example, to do feature selection (greedy algorithms, PCA, etc.) and then there are even different versions and packages for the same machine learning algorithms (e.g. Logistic Regression).
                                What method do I then use to tune hyperparameters (Bayesian Optimization, Random, TuneGrid, etc.)? How do I know which approach is the best for this particular problem?
                                Once again, I tried not getting dragged down by the insignificant minutae of each model and approach whilst still remaining detail-oriented .
                            </p>  
